# NLP sem q-paper

---

**1. Discuss in details the steps of Natural Language processing. Discuss with example the various levels in ambiguity in Natural Language? (10 points)**

---

**2. Explain the algorithm to edit one string X of length n to a string Y of length m. Show the steps of your algorithm for X=INTENTION and Y=EXECUTION (10 points)**

---

**3. i) What is bag of Words? (2 points)**
   **ii) What is TF-IDF? Mention its application (3 points)**

---

**4. i) Define morphemes (1 point)**
   **ii) Discuss two levels of morphology with suitable examples (4 points)**

---

**5. Discuss with example the various levels in ambiguity in Natural Language? (Module 1/CO1/Understand-LOCQ) (5 points)**

---

**6. Explain Lemmatization and Stemming with example (Module 3/CO2/Understand-IOCQ) (5 points)**

---

**7. (i) What are the advantages of using higher-order N-grams (e.g., trigrams, 4-grams) compared to lower-order N-grams (e.g., bigrams) in language modeling? (Module4/CO3/Analyse-IOCQ) (3 points)**
   **(ii) Suppose we have a corpus of text, and we want to calculate the bigram probability of the word "apple" following the word "green" based on the following data: In our corpus, we find that the word "green" occurs 50 times. Out of those 50 occurrences of "green," the word "apple" follows "green" in 10 instances. (Module 4/CO3/Apply-IOCQ) (2 points)**

---

**8. How do homonyms create ambiguity in language, and why is disambiguation important? (i) Illustrate with suitable examples the different levels in NLP (5 points) (ii) List and explain challenges of natural language processing (5 points) (iii) How do you evaluate the performance of an NLP model? (5 points)**

---

**9. What are stopwords? Give example (3 points)**

---

**10. i) What is a regular expression (3 points)**
    **ii) Write regular expressions for the following languages: a) the set of all alphabetic strings; b) the set of all lowercase alphabetic strings ending in a b; c) the set of all strings from the alphabet a, b such that each a is immediately preceded by and immediately followed by a b; d) 4¬∑3 Gb (8 points) iii) Differentiate between top-down and bottom-up parsing (3 points)**

---

**11. How do homonyms create ambiguity in language, and why is disambiguation important? (Module 7/CO2/Analyse-IOCQ) (5 points)**

---

**12. i) What is a corpus? (2 points)**
    **ii) Consider the following corpus C3 of 3 sentences. there is a big garden children play in a garden they play inside beautiful garden a) Calculate P(they play in a big garden) assuming a bi-gram language model (6 points) b) Calculate the perplexity of `<s>` they play in a big garden <\s>. (2 points) P(they | `<s>` ) = 1/ P(play | they) = 1/ P(in | play) = 1/ P(a | in) = 1/ P(big | a) = 1/ P(garden | big) = 1/ P(<\s>|garden) = 3/ P(they play in a big garden) = 1/3 x 1/1 x 1/2 x 1/1 x 1/2 x 1/1 x 3/3 = 1/ ùëÉùëÉùëÉùëÉùëÉùëÉùëÉùëÉùëÉùëÉùëÉùëÉùëÉùëÉùëÉùëÉùëÉùëÉùëÉùëÉùëÉ=‚àö^712 =1. iii) What are the advantages of using higher-order N-grams (e.g., trigrams, 4-grams) compared to lower-order N-grams (e.g., bigrams) in language modeling?**

---

**13. i) What do you mean by Part-of-Speech tagging? (4 points) ii) Compare open class & closed class word groups with suitable examples (4 points) iii) What is Word Sense Disambiguation (WSD)? (3 points) iv) Explain how Transformation-Based Learning (TBL) Rules are applied in NLP (4 points)**

---

**14. For each sentence, identify whether the different meanings arise from structural ambiguity, syntactic ambiguity, semantic ambiguity, or pragmatic ambiguity? (i) Time flies like an arrow (ii) He crushed the key to my heart (Module1/CO1/Apply-HOCQ) (2.5+2.5=5)**

---

**15. i) Discuss two-level Morphology with suitable example (Module 3/CO2/Understand-IOCQ) (5 points) ii) Discuss the differences between Inflectional Morphology and Derivational Morphology with suitable example (Module 3/CO2/Analyse-IOCQ) (5 points) iii) Apply the appropriate rule of Porter's Algorithm to convert the following: (a) CHARACTERIZATION ‚Üí CHARACTER (Module 3/CO2/Apply-IOCQ) (5 points) (b) MULTIDIMENSIONAL ‚Üí MULTIDIMENSION**

---

**16. i) Draw tree structure for the following ATIS sentences : (Module 6/CO2/Apply-HOCQ) (5 points) I prefer a morning flight I want a morning flight Using S ‚ÜíNP VP NP ‚ÜíPronoun | Pronoun-Noun | Det Nominal Nominal ‚ÜíNominal Noun |Noun VP ‚Üíverb | Verb NP | Verb NP PP | Verb PP (ii) Why is synonymy important in language and communication? (Module 7/CO2/Analyse-IOCQ) (5 points) (iii) What is perplexity in the context of language modeling, and why is it an important metric? (Module 4/CO2/Analyse-IOCQ) (5 points)**

---

**17. (i) Briefly describe Hidden Markov Model (HMM). (Module 6/CO4/Apply-HOCQ) (5 points) (ii) How does the Viterbi algorithm work in the context of hidden Markov models (HMMs)? (Module 6/CO4/Apply-HOCQ) (5 points)**

---

**18. (i) What is query term proximity, and how does it impact the effectiveness of search engine results? (Module 8/CO5/Analyse-IOCQ) (5 points) (ii) How does query optimization impact the efficiency of natural language processing applications? (Module 8/CO5/Analyse-IOCQ)**

---

**19. Write the application of Hidden Markov Model (HMM) in Parts-of-speech tagging. (Module 6/CO4/Apply-HOCQ) (5 points)**

---

**20. Relate how the Viterbi algorithm work in the context of hidden Markov models (HMMs)? (Module 6/CO4/Apply-HOCQ) (5 points)**

---

**21. Explain vector space model of information retrieval. (Module 8/CO5/Evaluate-IOCQ) (5 points)**

---

**22. a) Describe the fundamental concepts involved in Natural Language Processing (NLP). (Module1/CO1/Understand-LOCQ) b) How would you apply Natural Language Processing techniques to sentiment analysis in social media posts? (Module1/CO1/Apply-LOCQ) 2.5+2.5= c) What are the main challenges in NLP? (Module1/CO1/Understand-LOCQ) (3 points)**

---

**23. Explain with example Disjunction, Grouping and Precedence for pattern matching in regular expression? (Module3/CO2/Analyze-IOCQ) (5 points)**

---

**24. ‚ÄúPattern matching by regular expression is greedy.‚Äù Justify the statement. (Module3/CO2/Analyze-IOCQ) (5 points)**

---

**25. Design a regular expression to find all instances of the word ‚Äòthe‚Äô in a text. (Module3/CO2/Apply-IOCQ) (5 points)**

---

**26. Describe the different steps for building the NLP pipeline. (Module1/CO1/Understand-LOCQ) (5 points)**

---

**27. Describe how lemmatization is done? (Module3/CO2/Understand-LOCQ) (5 points)**

---

**28. What are the operations of editing one string to another? Explain. (Module3/CO2/Understand-LOCQ) (5 points)**

---

**29. Calculate edit distance from leda to deal. (Module3/CO2/Apply-IOCQ) (5 points)**

---

**30. Give two instances of real-world NLP uses. (Module1/CO1/Understand-LOCQ) (3 points)**

---

**31. What are the first few steps you'll take before applying a natural language processing (NLP) machine-learning algorithm on a corpus? (Module1/CO1/Understand-LOCQ) Answer: ÔÉò Eliminating white spaces ÔÉò Eliminating Punctuation ÔÉò Lowercase to Uppercase Conversion ÔÉò Tokenization ÔÉò Getting Rid of Stopwords ÔÉò Lemmatization. (5 points)**

---

**32. Computing minimum edit distances by hand, figure out whether drive is closer to brief or to divers and what the edit distance is. [Using 1-insertion, 1-deletion, 2-substitution costs] (Module2/CO2/Apply-IOCQ) (7 points)**

---

**33. Design an FSA which will accept the words for English numbers 1-99. (Module3/CO2/Apply-IOCQ) (5 points)**

---

**34. Write an FSA for time-of-day expressions like eleven o‚Äôclock, twelve-thirty, midnight, or a quarter to ten, and others. (Module3/CO2/Apply-IOCQ) (7 points)**

---

**35. Define Types and Tokens. (Module2/CO2/Understand-LOCQ) (5 points) How many types and tokens are there in the following sentences. ‚ÄúThey picnicked by the pool, then lay back on the grass and looked at the stars‚Äù. (Module2/CO2/Apply-LOCQ)**

---

**36. Does Porter stemming algorithm improve information retrieval in search engines? Justify. (Module3/CO2/Analyze-IOCQ) (5 points)**

---

**37. Write down the equation for trigram probability estimation. (Module 4/CO3/Apply-IOCQ) (6 points)**

---

**38. How are N-gram models applied in text prediction and generation tasks, and what are some common challenges they face? (Module 4/CO3/Analyse-IOCQ) (5 points)**

---

**39. What are the advantages of using higher-order N-grams (e.g., trigrams, 4-grams) compared to lower-order N-grams (e.g., bigrams) in language modeling? (Module 4/CO3/Analyse-IOCQ) (5 points)**

---

**40. You have trained a bigram language model on a large corpus of text. In this model, the vocabulary contains 5,000 words. You want to calculate the probability of the sentence "I love AI" according to this bigram model. You have the following information: The probability of the word "I" occurring at the beginning of a sentence (i.e., P("I" | `<start>`)) is 0.05. The bigram probabilities for the words "love" and "AI" are as follows: P("love" | "I") = 0. P

**41. (i) What is Smoothing? Why is it required? (Module 4/CO3/Understand-IOCQ) (4 points)**
   **(ii) Write down the equation for the discount d = c*/c for add-one smoothing. (Module 4/CO3/Apply-IOCQ) (5 points)**
   **(iii) Do the same thing used for Witten - Bell smoothing. How do they differ? (Module 4/CO3/Analyse-IOCQ) (4 points)**

---

**42. What is perplexity in the context of language modeling, and why is it an important metric? (Module 4/CO2/Analyse-IOCQ) (5 points)**

---

**43. Why POS (Part-of-Speech) Tagging is required in NLP (Natural Language Processing)? (Module 6/CO2/Analyse-IOCQ) (5 points)**

---

**44. Find one tagging error in each of the following sentences that are tagged with The Penn Treebank tagset: (Module 6/CO2/Apply-HOCQ) (6 points) 1. I/PRP need/VBP a/DT flight/NN from/IN Atlanta/NN Atlanta/NNP 2. Does/VBZ this/DT flight/NN serve/VB dinner/NNS dinner/NN 3. I/PRP have/VB a/DT friend/NN living/VBG in/IN Denver/NNP have/VBP 4. Can/VBP you/PRP list/VB the/DT nonstop/JJ afternoon/NN flights/NNS Can/MD**

---

**45. Use the Penn Treebank tagset to tag each word in the following sentences from Damon Runyon‚Äôs short stories. You may ignore punctuation. 1. It is a nice night. It/PRP is/VBZ a/DT nice/JJ night/NN ./. 2. This crap game is over a garage in Fifty-second Street... This/DT crap/NN game/NN is/VBZ over/IN a/DT garage/NN in/IN Fifty-second/NNP Street/NNP... 3... .Nobody ever takes the newspapers she sells... .. .Nobody/NN ever/RB takes/VBZ the/DT newspapers/NNS she/PRP sells/VBZ... 4. He is a tall, skinny guy with a long, sad, mean-looking kisser, and a mournful voice. He/PRP is/VBZ a/DT tall/JJ ,/, skinny/JJ guy/NN with/IN a/DT long/JJ ,/, sad/JJ ,/, mean-looking/JJ kisser/NN ,/, and/CC a/DT mournful/JJ voice/NN ./.**

---

**46. Why is synonymy important in language and communication? (Module 7/CO2/Analyse-IOCQ) (4 points)**

---

**47. What is homonymy in linguistics, and how does it differ from polysemy? (Module 7/CO2/Analyse-IOCQ) (5 points)**

---

**48. Explain polysemy with suitable example. (Module 7/CO2/Understand-IOCQ) (3 points)**

---

**49. Define Prior probability and likelihood probability using Bayesian Method. (Module 5/CO4/Understand-IOCQ) (5 points)**

---

**50. What is Confusion Matrix? Why is it required in Natural Language Processing? (Module 5/CO4/Understand-IOCQ) (2+ points)**
