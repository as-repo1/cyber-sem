###   1. Discuss in detail the steps of Natural Language Processing. Discuss with examples the various levels of ambiguity in Natural Language. (10 points)

**Natural Language Processing Steps:**

1. **Tokenization:** Breaking down text into words or phrases.
2. **Part-of-Speech Tagging:** Assigning grammatical categories to each token.
3. **Parsing:** Analyzing sentence structure to determine relationships between words.
4. **Named Entity Recognition (NER):** Identifying entities like names, locations, and organizations.
5. **Sentiment Analysis:** Determining the sentiment expressed in the text.
6. **Coreference Resolution:** Resolving references to the same entity.
7. **Semantic Role Labeling (SRL):** Identifying the roles of words in a sentence.
8. **Word Sense Disambiguation (WSD):** Resolving ambiguity in word meanings.
9. **Text Generation:** Creating coherent and contextually relevant text.
10. **Machine Translation:** Translating text from one language to another.

**Levels of Ambiguity:**

- **Lexical Ambiguity:** Words with multiple meanings. Example: "Bank" (financial institution or river bank).
- **Syntactic Ambiguity:** Ambiguity in sentence structure. Example: "I saw the man with the telescope" (Did I use the telescope or did the man have it?).
- **Semantic Ambiguity:** Ambiguity in the meaning of words or phrases. Example: "She saw the man on the hill with the telescope" (Who has the telescope?).
- **Pragmatic Ambiguity:** Ambiguity in the implied meaning due to context. Example: "Can you pass me the salt?" (Could mean passing the salt physically or verbally).

### 2. Explain the algorithm to edit one string X of length n to a string Y of length m. Show the steps of your algorithm for X=INTENTION and Y=EXECUTION (10 points)

**Algorithm: Edit Distance (Dynamic Programming)**

1. **Initialization:** Create an (n+1) x (m+1) matrix.
2. **Base Cases:** Fill in the first row and column with values 0 to n and 0 to m respectively.
3. **Dynamic Programming:** Fill in the matrix using recurrence relation:
   - If X[i] equals Y[j], then D[i][j] = D[i-1][j-1].
   - Otherwise, D[i][j] = min(D[i-1][j], D[i][j-1], D[i-1][j-1]) + 1.
4. **Backtracking:** Trace the matrix back to find the operations that lead to the minimum edit distance.

**Example:**
For X=INTENTION and Y=EXECUTION:

- Matrix Initialization:

```
  |   | E | X | E | C | U | T | I | O | N |
--+---+---+---+---+---+---+---+---+---+---+
  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |
I | 1 |
N | 2 |
T | 3 |
E | 4 |
N | 5 |
T | 6 |
I | 7 |
O | 8 |
N | 9 |
```

- Dynamic Programming:

```
  |   | E | X | E | C | U | T | I | O | N |
--+---+---+---+---+---+---+---+---+---+---+
  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |
I | 1 | 1 | 2 | 3 | 4 | 5 | 6 | 5 | 6 | 7 |
N | 2 | 2 | 2 | 3 | 4 | 5 | 6 | 6 | 6 | 7 |
T | 3 | 3 | 3 | 3 | 4 | 5 | 5 | 6 | 7 | 7 |
E | 4 | 3 | 4 | 4 | 4 | 5 | 6 | 7 | 7 | 8 |
N | 5 | 4 | 4 | 5 | 5 | 5 | 6 | 7 | 8 | 8 |
T | 6 | 5 | 5 | 5 | 5 | 6 | 6 | 7 | 8 | 9 |
I | 7 | 6 | 6 | 6 | 6 | 6 | 7 | 6 | 7 | 8 |
O | 8 | 7 | 7 | 7 | 7 | 7 | 8 | 7 | 6 | 7 |
N | 9 | 8 | 8 | 8 | 8 | 8 | 9 | 8 | 7 | 6 |
```

- Backtracking: Follow the path with the minimum cost, giving the operations. In this case, "Substitute I with E", "Do nothing", "Do nothing", "Do nothing", "Do nothing", "Substitute N with O", "Do nothing", "Delete T", "Do nothing".

### 3. (i) What is bag of Words? (2 points) (ii) What is TF-IDF? Mention its application (3 points)

**i) Bag of Words (BoW):**

- **Definition:** BoW is a representation of text that ignores the order and structure but considers the frequency of words.
- **Example:** For the sentence "The cat in the hat," the BoW representation would be: {The: 2, cat: 1, in: 1, the: 1, hat: 1}.

**ii) TF-IDF (Term Frequency-Inverse Document Frequency):**

- **Definition:** TF-IDF is a numerical statistic that reflects the importance of a word in a document relative to a collection of documents (corpus).
- **Formula:** TF-IDF = TF (Term Frequency) * IDF (Inverse Document Frequency).
- **Application:** It is widely used in information retrieval and text mining tasks. For example, in document classification, TF-IDF helps in identifying the importance of words in distinguishing one category from another.

This helps to weigh down the impact of common words and highlight words that are distinct to a particular document.
