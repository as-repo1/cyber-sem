### 1. Discuss in detail the steps of Natural Language Processing. Discuss with examples the various levels of ambiguity in Natural Language. (10 points)

### **Natural Language Processing Steps:**

#### 1. Lexical Analysis:

- **Definition:** Lexical analysis involves deciphering and segmenting language into meaningful units, such as paragraphs, sentences, phrases, and words.
- **Process:**
  - Identifying lexemes: The text is broken down into lexemes, which are the basic units of meaning.
  - Categorizing into parts of speech (POS): Words are classified based on their grammatical functions.
  - Identifying morphemes: Lexemes are further divided into morphemes, the smallest units of meaning.
- **Example:** Analyzing the sentence "the understandable vocabulary that makes up a language" involves identifying lexemes like "vocabulary" and "language," categorizing them as nouns, and breaking them into morphemes like "understand-" and "-able."

#### 2. Syntactic Analysis:

- **Definition:** Syntactic analysis examines how words and phrases arrange to form grammatically correct sentences.
- **Process:**
  - Checking grammar: Evaluating the arrangement of words for proper syntax.
- **Example:** The sentence "Dave wrote the paper" passes syntactic analysis, while "Dave do jumps" is deemed syntactically incorrect.

#### 3. Semantic Analysis:

- **Definition:** Semantic analysis aims to understand the literal meaning of individual language selections, focusing on the meaning of words, phrases, and sentences.
- **Process:**
  - Understanding meaning: Assessing the actual meaning of language selections.
- **Example:** "Manhattan calls out to Dave" might pass syntactic analysis but fails semantic analysis because it doesn't make literal sense for a place to call out to someone.

#### 5. Pragmatic Analysis:

- **Definition:** Pragmatic analysis focuses on interpreting the intended meaning rather than the literal meaning of language.
- **Process:**
  - Deriving intended meaning: Assessing the context to understand the speaker's intended meaning.
- **Example:** Understanding that "Manhattan speaks to all its people" is likely a metaphorical expression rather than a literal statement.

#### 4. Discourse Integration:

- **Definition:** Discourse integration involves analyzing prior words and sentences to understand the meaning of ambiguous language.
- **Process:**
  - Contextual analysis: Understanding the context by considering preceding words and sentences.
- **Example:** If one sentence mentions "Manhattan speaks to all its people," discourse integration helps understand that "It" in the following sentence refers to Manhattan.

### **Levels of Ambiguity:**

- **Lexical Ambiguity:** Words with multiple meanings. Example: "Bank" (financial institution or river bank).
- **Syntactic Ambiguity:** Ambiguity in sentence structure. Example: "I saw the man with the telescope" (Did I use the telescope or did the man have it?).
- **Semantic Ambiguity:** Ambiguity in the meaning of words or phrases. Example: "She saw the man on the hill with the telescope" (Who has the telescope?).
- **Pragmatic Ambiguity:** Ambiguity in the implied meaning due to context. Example: "Can you pass me the salt?" (Could mean passing the salt physically or verbally).

### 2. Explain the algorithm to edit one string X of length n to a string Y of length m. Show the steps of your algorithm for X=INTENTION and Y=EXECUTION (10 points)

**Algorithm: Edit Distance (Dynamic Programming)**

1. **Initialization:** Create an (n+1) x (m+1) matrix.
2. **Base Cases:** Fill in the first row and column with values 0 to n and 0 to m respectively.
3. **Dynamic Programming:** Fill in the matrix using recurrence relation:
   - If X[i] equals Y[j], then D[i][j] = D[i-1][j-1].
   - Otherwise, D[i][j] = min(D[i-1][j], D[i][j-1], D[i-1][j-1]) + 1.
4. **Backtracking:** Trace the matrix back to find the operations that lead to the minimum edit distance.


### 3. (i) What is bag of Words? (2 points) (ii) What is TF-IDF? Mention its application (3 points)

**i) Bag of Words (BoW):**

- **Definition:** BoW is a representation of text that ignores the order and structure but considers the frequency of words.
- **Example:** For the sentence "The cat in the hat," the BoW representation would be: {The: 2, cat: 1, in: 1, hat: 1}.

**ii) TF-IDF (Term Frequency-Inverse Document Frequency):**

- **Definition:** TF-IDF is a numerical statistic that reflects the importance of a word in a document relative to a collection of documents (corpus).
- **Formula:** TF-IDF = TF (Term Frequency) * IDF (Inverse Document Frequency).
- **Application:** It is widely used in information retrieval and text mining tasks. For example, in document classification, TF-IDF helps in identifying the importance of words in distinguishing one category from another.

This helps to weigh down the impact of common words and highlight words that are distinct to a particular document.

### 4. (i) Define morphemes (1 point) (ii) Discuss two levels of morphology with suitable examples (4 points)

**i) Morphemes:**

- **Definition:** Morphemes are the smallest units of meaning in a language. They can be words or parts of words that carry semantic significance.
- **Example:** In the word "unhappiness," "un-" and "-ness" are morphemes. "Un-" denotes negation, and "-ness" denotes a state or quality.

**ii) Two Levels of Morphology:**

1. **Free Morphemes:**

   - **Definition:** Can stand alone as complete words.
   - **Example:** In the sentence "She is happy," both "she" and "happy" are free morphemes.
2. **Bound Morphemes:**

   - **Definition:** Cannot stand alone and must attach to a free morpheme.
   - **Example:** In the word "happiness," "-ness" is a bound morpheme attached to the free morpheme "happy."

### 5. Discuss with an example the various levels of ambiguity in Natural Language? (Module 1/CO1/Understand-LOCQ) (5 points)

**Levels of Ambiguity in Natural Language:**

1. **Lexical Ambiguity:**

   - **Example:** "The bank is closed." (Is it a financial institution or the side of a river?)
2. **Syntactic Ambiguity:**

   - **Example:** "I saw the man with the telescope." (Did I use the telescope or did the man have it?)
3. **Semantic Ambiguity:**

   - **Example:** "She saw the man on the hill with the telescope." (Who has the telescope?)
4. **Pragmatic Ambiguity:**

   - **Example:** "Can you pass me the salt?" (Could mean physically passing the salt or asking someone to pass it verbally.)

### 6. Explain Lemmatization and Stemming with an example (Module 3/CO2/Understand-IOCQ) (5 points)

**Lemmatization:**

- **Definition:** Lemmatization is the process of reducing a word to its base or root form.
- **Example:** For the word "running," the lemma is "run." It considers context and the word's part of speech.

**Stemming:**

- **Definition:** Stemming is the process of removing suffixes to obtain a word's root form.
- **Example:** For the word "running," the stem is "run." It is a more heuristic approach and may not always result in a valid word.

### 7. (i) What are the advantages of using higher-order N-grams compared to lower-order N-grams in language modeling? (Module4/CO3/Analyse-IOCQ) (3 points) (ii) Calculate the bigram probability of the word "apple" following the word "green" based on the given data. (Module 4/CO3/Apply-IOCQ) (2 points)

**i) Advantages of Higher-Order N-grams:**

- **Greater Contextual Information:** Higher-order N-grams capture more extended contexts, providing a better understanding of the language structure.
- **Improved Accuracy:** By considering more preceding words, higher-order N-grams can better handle language nuances and improve predictive accuracy.
- **Reduced Ambiguity:** Longer contexts help in disambiguating between words with multiple meanings.

**ii) Bigram Probability Calculation:**

- **Given Data:**
  - Count of "green": 50
  - Count of "green" followed by "apple": 10
- **Bigram Probability:**
  - P("apple" | "green") = Count("green apple") / Count("green")
  - P("apple" | "green") = 10 / 50
  - P("apple" | "green") = 0.2

### 8. How do homonyms create ambiguity in language, and why is disambiguation important? (i) Illustrate with suitable examples the different levels in NLP (5 points) (ii) List and explain challenges of natural language processing (5 points) (iii) How do you evaluate the performance of an NLP model? (5 points)

**i) Homonyms and Ambiguity:**

- **Example:** The word "bat" can refer to a flying mammal or a piece of sports equipment. In the sentence "I saw a bat," the meaning is unclear without context.

**ii) Challenges in NLP:**

1. **Ambiguity:** Dealing with various types of ambiguity in language.
2. **Data Quality:** Ensuring the quality and reliability of training data.
3. **Domain Adaptation:** Adapting models to different domains and contexts.
4. **Computational Complexity:** Processing large amounts of text efficiently.
5. **Lack of Context:** Resolving meaning without sufficient context.

**iii) Evaluation of NLP Model:**

- **Metrics:** Precision, recall, F1 score, accuracy.
- **Human Evaluation:** Expert judgment on model outputs.
- **Cross-Validation:** Testing model performance on different data splits.
- **Perplexity:** Measure of model uncertainty in language modeling.
