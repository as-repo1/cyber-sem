### **31. Steps Before Applying NLP Machine-Learning Algorithm on a Corpus**

Before applying an NLP machine-learning algorithm to a corpus, several preprocessing steps are typically performed:

1. **Eliminating White Spaces:** Remove unnecessary spaces to ensure uniformity and readability.
2. **Eliminating Punctuation:** Strip punctuation marks to focus on the textual content.
3. **Lowercase to Uppercase Conversion:** Convert all text to lowercase to treat words with different cases as the same.
4. **Tokenization:** Break down the text into individual words or tokens for analysis.
5. **Getting Rid of Stopwords:** Remove common words (stopwords) that don't contribute much to the overall meaning.
6. **Lemmatization:** Reduce words to their base or root form for better analysis.

These steps help clean and prepare the corpus for effective machine-learning algorithms, improving accuracy and efficiency.

### **32. Computing Minimum Edit Distances for "drive" and "brief" or "divers"**

Minimum Edit Distance for "drive" to "brief":

* remove 'v' - drie
* insert 'f' - drief
* replace 'd' with 'b' -brief
  * total distance - 3

Minimum Edit Distance for "drive" to "divers":

* remove 'r' - dive
* insert 'r' and 's' - divers (2steps)
  * total distance - 3

Both "brief" and "divers" have the same corrected minimum edit distance of 3 from "drive." I hope this provides a clear step-by-step breakdown.

### **33. Design an FSA for English Numbers 1-99**

\[ FSA \]
States: q0 (initial), q1 (final)
Alphabet: {1, 2, ..., 9, 10, ..., 90, 91, ..., 99}

Transitions:

- From q0 to q1 on input 1, 2, ..., 9 (transition for units)
- From q1 to q1 on input 10, 20, ..., 90 (transition for tens)
- From q1 to q1 on input 1, 2, ..., 9 (transition for units after tens)

Acceptance: q1 (final state) is an accepting state.

This FSA accepts English numbers from 1 to 99.

### **34. FSA for Time-of-Day Expressions**

\[ FSA \]
States: q0 (initial), q1, q2, q3, q4, q5, q6 (final)
Alphabet: {one, two, ..., twelve, quarter, half, o'clock, midnight, noon}

Transitions:

- From q0 to q1 on input {one, two, ..., twelve} (transition for hours)
- From q1 to q2 on input {o'clock} (transition for o'clock)
- From q2 to q3 on input {midnight, noon} (transition for special cases)
- From q1 to q4 on input {quarter, half} (transition for quarter/half past)
- From q4 to q5 on input {to} (transition for quarter/half to)
- From q5 to q6 on input {one, two, ..., twelve} (transition for hours after "to")

Acceptance: q6 (final state) is an accepting state.

This FSA accepts time-of-day expressions like "eleven o'clock," "twelve-thirty," "midnight," or "a quarter to ten."


### **35. Types and Tokens**

- **Definition:**

  - **Token:** A token is an instance of a sequence of characters in a particular utterance or written text. It is an individual occurrence of a linguistic unit.
  - **Type:** A type is a class of items that share common characteristics. In linguistics, it refers to a unique, distinct form or element.
- **Example:**

  - Sentence: "They picnicked by the pool, then lay back on the grass and looked at the stars."
  - **Tokens:** They, picnicked, by, the, pool, then, lay, back, on, the, grass, and, looked, at, the, stars (16 tokens)
  - **Types:** They, picnicked, by, the, pool, then, lay, back, on, grass, and, looked, at, stars (14 types)

### **36. Porter Stemming Algorithm improves Information Retrieval**

The Porter stemming algorithm aims to reduce words to their base or root form, which can improve information retrieval in search engines. However, its impact depends on the context and the specific goals of the information retrieval system.

**Justification:**

- **Advantages:**

  - **Normalization:** Stemming reduces words to their root, normalizing different forms of a word.
  - **Increased Recall:** It helps in capturing variations of a word, increasing recall in information retrieval.
- **Challenges:**

  - **Overstemming and Understemming:** Porter stemming may overstem (reduce unrelated words to the same stem) or understem (leave related words unstemmed).
  - **Semantic Loss:** Stemming may lead to the loss of semantic meaning, impacting precision.

The decision to use Porter stemming depends on the trade-off between recall and precision in a particular information retrieval context.

### **37. Equation for Trigram Probability Estimation**

- ![1702891446612](image/q31-40/1702891446612.png)

### **38. N-gram Models in Text Prediction and Challenges**

- **N-gram models in Text Prediction and Generation:**

N-gram models are widely used in natural language processing tasks, particularly in text prediction and generation. An N-gram is a contiguous sequence of N items from a given sample of text or speech. Here's how N-gram models are applied in these tasks:

1. **Text Prediction:**

   - **Training Phase:** In the training phase, the N-gram model analyzes a large corpus of text to estimate the probability of occurrence of each N-gram.
   - **Probabilities:** It calculates probabilities of the next word given the context of the previous N-1 words. For example, a bigram model (N=2) calculates the probability of the current word given the previous word.
   - **Storage:** These probabilities are stored in a table or model that can be used during prediction.
2. **Text Generation:**

   - **Seed Text:** The process begins with a seed text, which can be a few words or sentences.
   - **N-gram Prediction:** The model uses the seed text to predict the next word or sequence of words based on the highest probability from the N-gram model.
   - **Iteration:** This predicted word(s) is then added to the seed text, and the process is iteratively repeated to generate longer text.

**Challenges:**

1. **Data Sparsity:**

   - **Issue:** As N increases, the number of possible N-grams grows exponentially, leading to sparse data.
   - **Challenge:** Sparse data makes it challenging to accurately estimate probabilities for less frequent N-grams.
2. **Fixed Context Size:**

   - **Issue:** N-gram models use a fixed context size (N-1 words).
   - **Challenge:** Capturing long-range dependencies in language requires a large N, but it increases sparsity and computational complexity.
3. **Out-of-Vocabulary Words:**

   - **Issue:** Words not present in the training corpus can't be predicted.
   - **Challenge:** Handling out-of-vocabulary words and adapting to new vocabulary is essential for real-world applications.
4. **Context Sensitivity:**

   - **Issue:** N-gram models may struggle with understanding context nuances.
   - **Challenge:** Capturing semantics and context beyond a fixed window is challenging.
5. **Evaluation Metrics:**

   - **Issue:** Determining the effectiveness of N-gram models.
   - **Challenge:** Identifying suitable evaluation metrics that consider both fluency and relevance in generated text.

Addressing these challenges often involves using smoothing techniques, considering more sophisticated language models, and incorporating deep learning approaches like recurrent neural networks (RNNs) and transformers for improved performance.


### **39. Advantages of Higher-Order N-grams in Language Modeling**

- **Richer Context:**

  - Higher-order N-grams consider a more extensive context of preceding words, providing a richer linguistic context for modeling.
- **Improved Precision:**

  - By incorporating additional words into the context, higher-order N-grams can capture more intricate relationships and nuances in language, leading to improved precision.
- **Handling Ambiguity:**

  - Language often exhibits ambiguity that can be resolved with a broader context. Higher-order N-grams are better equipped to disambiguate between different interpretations.
- **Long-Term Dependencies:**

  - For tasks requiring an understanding of long-term dependencies between words, higher-order N-grams are more effective in maintaining context over a more extended sequence.
- **Reduced Data Sparsity Issues:**

  - With a larger context, higher-order N-grams mitigate data sparsity problems common in lower-order models, where the frequency of specific sequences may be limited.

### **40. Bigram Language Model Calculation**

- multiply them,

    0.05 x 0.4 x 0.2 = 0.004 (ans)
